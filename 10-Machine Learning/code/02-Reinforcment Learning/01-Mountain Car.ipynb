{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Action Cart\n",
    "\n",
    "Mountain car actions:\n",
    "\n",
    "* 0 - Apply left force\n",
    "* 1 - Apply no force\n",
    "* 2 - Apply right force\n",
    "\n",
    "State values:\n",
    "\n",
    "* state[0] - Position \n",
    "* state[1] - Velocity\n",
    "\n",
    "The following shows a cart that simply applies full-force to climb the hill.  The cart is simply not strong enough.  It will need to use momentum from the hill behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Q^{new}(s_{t},a_{t}) \\leftarrow (1-\\alpha) \\cdot \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} \\bigg) }^{\\text{learned value}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "DISCRETE_GRID_SIZE = [10, 10]\n",
    "buckets = (env.observation_space.high - env.observation_space.low)/DISCRETE_GRID_SIZE\n",
    "\n",
    "def calc_discrete_state(state):\n",
    "    discrete_state = (state - env.observation_space.low)/buckets\n",
    "    return tuple(discrete_state.astype(np.int))\n",
    "\n",
    "def show_q_table(q_table):\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(q_table.argmax(axis=2))\n",
    "    df.columns = [f'v-{x}' for x in range(DISCRETE_GRID_SIZE[0])]\n",
    "    df.index = [f'p-{x}' for x in range(DISCRETE_GRID_SIZE[1])]\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.5\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 80\n",
    "\n",
    "START_EPSILON_DECAYING = 1\n",
    "END_EPSILON_DECAYING = EPISODES//2\n",
    "\n",
    "epsilon = 1  \n",
    "epsilon_change = epsilon/(END_EPSILON_DECAYING - START_EPSILON_DECAYING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v-0  v-1  v-2  v-3  v-4  v-5  v-6  v-7  v-8  v-9\n",
      "p-0    0    0    0    0    0    0    0    0    0    0\n",
      "p-1    0    0    0    0    0    0    0    0    0    0\n",
      "p-2    0    0    0    0    0    0    0    0    0    0\n",
      "p-3    0    0    0    0    0    0    0    0    0    0\n",
      "p-4    0    0    0    0    0    0    0    0    0    0\n",
      "p-5    0    0    0    0    0    0    0    0    0    0\n",
      "p-6    0    0    0    0    0    0    0    0    0    0\n",
      "p-7    0    0    0    0    0    0    0    0    0    0\n",
      "p-8    0    0    0    0    0    0    0    0    0    0\n",
      "p-9    0    0    0    0    0    0    0    0    0    0\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((DISCRETE_GRID_SIZE + [env.action_space.n]))\n",
    "show_q_table(q_table)\n",
    "\n",
    "\n",
    "# np.argmax(q_table[(2,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(q_table):\n",
    "    done = False\n",
    "    discrete_state = calc_discrete_state(env.reset())\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Exploit or explore\n",
    "        if np.random.random() > epsilon:\n",
    "            # Exploit - use q-table to take current best action (and probably refine)\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Explore - t\n",
    "            action = np.random.randint(0, env.action_space.n)\n",
    "            \n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    " \n",
    "        new_state_disc = calc_discrete_state(new_state)\n",
    "          \n",
    "        # Update q-table\n",
    "        max_future_q = np.max(q_table[new_state_disc])\n",
    "        current_q = q_table[discrete_state + (action,)]\n",
    "        new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "        q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "        discrete_state = new_state_disc\n",
    "        \n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 0\n",
    "\n",
    "while episode < EPISODES:\n",
    "    \n",
    "    run_game(q_table)\n",
    "\n",
    "    # Move epsilon towards its ending value, if it still needs to move\n",
    "    if END_EPSILON_DECAYING >= episode >= START_EPSILON_DECAYING:\n",
    "        epsilon -= epsilon_change\n",
    "        \n",
    "    episode += 1\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v-0  v-1  v-2  v-3  v-4  v-5  v-6  v-7  v-8  v-9\n",
      "p-0    0    0    0    2    0    2    2    0    0    0\n",
      "p-1    0    1    1    2    1    0    2    2    2    0\n",
      "p-2    0    2    0    0    1    2    2    2    2    0\n",
      "p-3    0    2    0    0    1    2    2    2    1    0\n",
      "p-4    2    1    1    0    0    0    2    1    2    1\n",
      "p-5    0    2    2    0    0    1    1    1    2    0\n",
      "p-6    0    1    0    1    2    1    2    2    2    0\n",
      "p-7    0    0    1    0    0    0    0    2    0    0\n",
      "p-8    0    0    0    1    2    1    0    1    0    0\n",
      "p-9    0    0    0    0    0    0    1    0    0    0\n"
     ]
    }
   ],
   "source": [
    "show_q_table(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Tensorflow GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
